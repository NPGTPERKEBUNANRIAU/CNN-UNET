{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D\n",
    "from keras.layers import  Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "from random import shuffle\n",
    "from osgeo import gdal\n",
    "\n",
    "from pyproj import Proj, transform\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path = \"D:/Kanwil Riau/Updating NPGT Perkebunan/OLAH TIM/RIAU\"\n",
    "images = sorted(glob(os.path.join(path, \"Image/*\")))\n",
    "masks = sorted(glob(os.path.join(path, \"Mask/*\")))\n",
    "\n",
    "minx = 0\n",
    "maxx = 65535\n",
    "\n",
    "a = images[1]\n",
    "b = gdal.Open(a).ReadAsArray()\n",
    "c = np.rollaxis(b, 0, 3)\n",
    "d = (c - minx) / (maxx - minx)\n",
    "#d = np.expand_dims(c, axis=-3)\n",
    "print(d.shape)\n",
    "\n",
    "e = masks[1]\n",
    "f = gdal.Open(e).ReadAsArray()\n",
    "g = np.expand_dims(f, axis=-1)\n",
    "g.shape\n",
    "\n",
    "#h = (g - minx) / (maxx - minx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def load_data(path, split=0.1):\n",
    "    images = sorted(glob(os.path.join(path, \"Image/*\")))\n",
    "    masks = sorted(glob(os.path.join(path, \"Mask/*\")))\n",
    "\n",
    "    total_size = len(images)\n",
    "    valid_size = int(split * total_size)\n",
    "    test_size = int(split * total_size)\n",
    "\n",
    "    train_x, valid_x = train_test_split(images, test_size=valid_size)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=valid_size)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=test_size)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=test_size)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    #path = path.decode()\n",
    "    x = gdal.Open(path).ReadAsArray()\n",
    "    x = np.rollaxis(x, 0, 3)\n",
    "    x = x.astype(np.float32)\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    y = gdal.Open(path).ReadAsArray()\n",
    "    #x[x == 2] = 1 \n",
    "    #x = np.rollaxis(x, 0, 3)\n",
    "    #x[np.isnan(x)] = 0\n",
    "    y = np.expand_dims(y, axis=-1)\n",
    "    y = y.astype(np.float32)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mask(mask, num_classes=13):\n",
    "    mask = tf.squeeze(mask, axis=-1)  # Menghapus dimensi channel jika ada\n",
    "    mask = tf.cast(mask, tf.int32)    # Pastikan mask dalam format integer\n",
    "    mask = tf.one_hot(mask, num_classes)  # One-hot encoding\n",
    "    mask = tf.cast(mask, tf.float32)  # Konversi ke float32\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_parse(x, y, num_classes=13):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        y = preprocess_mask(y, num_classes)  # Ensure masks are one-hot encoded\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape([1024, 1024, 4])\n",
    "    y.set_shape([1024, 1024, num_classes])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(x, y, batch=8, buffer_size=100, num_classes=13):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.map(lambda x, y: tf_parse(x, y, num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Input, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def unet(input_shape=(1024, 1024, 4), num_classes=13):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, (3, 3), padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(64, (3, 3), padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(128, (3, 3), padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, (3, 3), padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(256, (3, 3), padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, (3, 3), padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(512, (3, 3), padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    drop4 = Dropout(0.3)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, (3, 3), padding='same')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(1024, (3, 3), padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    drop5 = Dropout(0.3)(conv5)\n",
    "\n",
    "    # Decoder\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    up6 = Conv2D(512, (2, 2), padding='same')(up6)\n",
    "    merge6 = Concatenate()([drop4, up6])\n",
    "    conv6 = Conv2D(512, (3, 3), padding='same')(merge6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    conv6 = Conv2D(512, (3, 3), padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "\n",
    "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    up7 = Conv2D(256, (2, 2), padding='same')(up7)\n",
    "    merge7 = Concatenate()([conv3, up7])\n",
    "    conv7 = Conv2D(256, (3, 3), padding='same')(merge7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    conv7 = Conv2D(256, (3, 3), padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "\n",
    "    up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    up8 = Conv2D(128, (2, 2), padding='same')(up8)\n",
    "    merge8 = Concatenate()([conv2, up8])\n",
    "    conv8 = Conv2D(128, (3, 3), padding='same')(merge8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    conv8 = Conv2D(128, (3, 3), padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "\n",
    "    up9 = UpSampling2D(size=(2, 2))(conv8)\n",
    "    up9 = Conv2D(64, (2, 2), padding='same')(up9)\n",
    "    merge9 = Concatenate()([conv1, up9])\n",
    "    conv9 = Conv2D(64, (3, 3), padding='same')(merge9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(64, (3, 3), padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "\n",
    "    # Output Layer\n",
    "    output = Conv2D(num_classes, (1, 1), activation='softmax')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def iou_multiclass(num_classes=13):\n",
    "    def iou(y_true, y_pred):\n",
    "        # Mengambil prediksi dengan nilai tertinggi untuk setiap kelas\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        y_true = tf.argmax(y_true, axis=-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for i in range(num_classes):\n",
    "            # Menghitung intersection\n",
    "            intersection = tf.reduce_sum(tf.cast((y_pred == i) & (y_true == i), tf.float32))\n",
    "            # Menghitung union\n",
    "            union = tf.reduce_sum(tf.cast((y_pred == i) | (y_true == i), tf.float32))\n",
    "            \n",
    "            # Menghindari pembagian dengan nol\n",
    "            iou_class = tf.cond(tf.equal(union, 0), lambda: tf.constant(1.0), lambda: intersection / union)\n",
    "            iou_per_class.append(iou_class)\n",
    "        \n",
    "        # Menghitung rata-rata IoU untuk semua kelas\n",
    "        mean_iou = tf.reduce_mean(iou_per_class)\n",
    "        return mean_iou\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Dataset\n",
    "    path = \"D:/Kanwil Riau/Updating NPGT Perkebunan/OLAH TIM/RIAU\"\n",
    "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 8\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create TensorFlow datasets\n",
    "train_dataset = tf_dataset(train_x, train_y)\n",
    "valid_dataset = tf_dataset(valid_x, valid_y)\n",
    "test_dataset = tf_dataset(test_x, test_y, batch=1)  # Test dataset usually has batch size 1\n",
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet()\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "metrics = [\"accuracy\", tf.keras.metrics.Recall(), tf.keras.metrics.Precision(),iou_multiclass(num_classes=13)]\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\"Files/RIAU.keras\"),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    CSVLogger(\"Files/RIAU.csv\"),\n",
    "    TensorBoard(),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(train_x)//batch\n",
    "valid_steps = len(valid_x)//batch\n",
    "\n",
    "if len(train_x) % batch != 0:\n",
    "    train_steps += 1\n",
    "if len(valid_x) % batch != 0:\n",
    "    valid_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset,\n",
    "          validation_data=valid_dataset,\n",
    "          epochs=100,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=valid_steps,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_parse(mask):\n",
    "    mask = np.squeeze(mask)\n",
    "    mask = [mask, mask, mask]\n",
    "    mask = np.transpose(mask, (1, 2, 0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"NPGT_Riau.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "test_dataset = tf_dataset(test_x, test_y, batch=batch_size)\n",
    " \n",
    "test_steps = (len(test_x)//batch_size)\n",
    "if len(test_x) % batch_size != 0:\n",
    "    test_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset, steps=test_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
